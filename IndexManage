# 创建一个新的Index

PUT /test_index
{
  "settings": {
      "number_of_shards": 5,
      "number_of_replicas": 1
  },
  "mappings": {
      "employee": {
          "properties": {
              "about": {
                  "type": "text"
              },
              "age": {
                  "type": "long"
              },
              "first_name": {
                  "type": "text"
              },
              "last_name": {
                  "type": "text"
              },
              "interests": {
                  "type": "text"
              }
          }
      }
  }
}

# 从ElasticSearch 6.X版本开始，一个index下面只能有一个type

# number_of_shards: 
#     The number of primary shards that an index should have, which defaults to 5.
#     This setting cannot be changed after index creation.

# number_of_replicas
#     The number of replica shards (copies) that each primary shard should have, which defaults to 1. 
#     This setting can be changed at any time on a live index


# 删除一个存在的Index
DELETE /test_index


# 为Index定制Analyzer，名字叫custom_spanish_analyzer
PUT /test_custom_analyzer
{
    "settings": {
        "number_of_shards": 5,
        "number_of_replicas": 1,
        "analysis": {
            "analyzer": {
                "custom_spanish_analyzer": {
                    "type": "standard",
                    "stopwords": "_spanish_"
                }
            }
        }
    }
}
# custom_spanish_analyzer不是全局的，它只存在于这个索引中
# 测试定制的Analyzer
GET /test_custom_analyzer/_analyze
{
    "analyzer": "custom_spanish_analyzer", 
    "text": "El veloz zorro marrón"
}



# 创建自定制的Analyzer
PUT /test_create_custom_analyzer
{
    "settings": {
        "number_of_shards": 5,
        "number_of_replicas": 1,

        "analysis": {
            # 自定义一个char_filter，将&转换成and，marco转换成poype
            "char_filter": {
                "&_to_and": {
                    "type": "mapping",
                    "mappings": ["&=> and", "marco=> poype"]
                }
            },
            # 自定义一个token_filter，自定义了stopword列表
            "filter": {
                "my_stopwords": {
                    "type": "stop",
                    "stopwords": ["the", "a", "ab", "ba"]
                }
            },

            "analyzer": {
                # 定义自己的analyzer，使用了上面自定义的char_filter和token_filter
                "my_analyzer": {
                    "type": "custom",
                    "char_filter": ["html_strip", "_to_and"],
                    "tokenizer": "standard",
                    "filter": ["lowercase", "my_stopwords"]
                }
            }
        }
    }
    "mappings": {
        "my_type": {
            "properties": {
                "title": {
                    # 指定title字段使用自己定制的analyzer
                    "type": "text",
                    "analyzer": "my_analyzer"
                }
            }
        }
    }
}

GET /test_create_custom_analyzer/_analyze
{
    "analyzer": "my_analyzer", 
    "text": "The quick & brown fox ab ba, a marco"
}
# 在json中直接写注释是会报错的，真正执行的时候要删掉
# analyzer由Character filter, Tokenizer和Token filter组成。
# 一个Analyzer可以有0或多个character filter，但必须有且仅有一个tokenizer，也可以有任意数量的Token filter。


# 查询API，可以使用_source属性明确指定查询结果中只包含感兴趣的property
GET /test_create_custom_analyzer/my_type/_search
{
    "query": {
        "match_all": {}
    }, 
    "_source": ["content"]
}

# 测试的索引中包含title和content两个property，默认查询结果中同时包含所有的property，使用_source明确指定结果中只包含content property